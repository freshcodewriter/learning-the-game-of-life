{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "546Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ4Oesp3JPY2",
        "colab_type": "code",
        "outputId": "166ab825-8575-40ae-a87b-e772192786b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "char_lookup = {'0' : 0, '-' : 1}\n",
        "\n",
        "lines = []\n",
        "filepath = '/content/Data/Subchallenge2/SubC2_TEST_data.txt'\n",
        "\n",
        "with open(filepath) as fp:\n",
        "  line = fp.readline()\n",
        "  cnt = 1\n",
        "  while line:\n",
        "    print(\"Line {}: {}\".format(cnt, line.strip()))\n",
        "    line = fp.readline()\n",
        "    cell_state = line.split()\n",
        "    encoded = []\n",
        "    if (len(cell_state) > 1):\n",
        "      for c in cell_state[-1]:\n",
        "        if c in char_lookup:\n",
        "          encoded.append(char_lookup[c])   \n",
        "        else:\n",
        "          encoded.append(.5)    \n",
        "      lines.append(encoded)\n",
        "      cnt += 1\n",
        "print(cnt)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-69f18251d9c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/Data/Subchallenge2/SubC2_TEST_data.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Data/Subchallenge2/SubC2_TEST_data.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuhIgt-IUXQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "test_data = torch.FloatTensor(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPj_vF3JfRCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Model architecture'''\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "\n",
        "        # Initialize encoder layers\n",
        "        self.enc1 = nn.Linear(200, 128)\n",
        "        self.enc2 = nn.Linear(128, 64)\n",
        "        self.enc_z = nn.Linear(64, 16)\n",
        "\n",
        "        # Initialize decoder layers\n",
        "        self.dec1 = nn.Linear(16, 64)\n",
        "        self.dec2 = nn.Linear(64, 128)\n",
        "        self.dec3 = nn.Linear(128, 200)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = torch.tanh(self.enc1(x))\n",
        "        h2 = torch.tanh(self.enc2(h1))\n",
        "        latent = self.enc_z(h2)\n",
        "\n",
        "        # decode latent factor\n",
        "        g1 = torch.tanh(self.dec1(latent))\n",
        "        g2 = torch.tanh(self.dec2(g1))\n",
        "        g3 = torch.sigmoid(self.dec3(g2))\n",
        "\n",
        "        return latent, g3\n",
        "\n",
        "    # Loss function for a batch\n",
        "    def loss(self, batch, outputs):\n",
        "        ce_term_1 = batch * torch.log(outputs)\n",
        "        ce_term_2 = (torch.ones_like(batch) - batch) * torch.log((torch.ones_like(outputs) - outputs))\n",
        "        ev = torch.sum(ce_term_1 + ce_term_2)\n",
        "\n",
        "        # kl = -.5 * torch.sum((1 + torch.log(torch.pow(std_vec, 2)) - torch.pow(m_vec, 2) - torch.pow(std_vec, 2)))\n",
        "\n",
        "        return ev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJombEwcrhHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = []\n",
        "import os\n",
        "cnt=0\n",
        "path = '/content/Data/Subchallenge2/SubC2_train_TXT'\n",
        "for r, d, f in os.walk(path):\n",
        "    for file in f:\n",
        "        if '.txt' in file:\n",
        "          with open(path + '/' + file) as fp:\n",
        "            line = fp.readline()\n",
        "            while line:\n",
        "              # print(\"Line {}: {}\".format(cnt, line.strip()))\n",
        "              line = fp.readline()\n",
        "              cell_state = line.split()\n",
        "              encoded = []\n",
        "              if (len(cell_state) > 1):\n",
        "                for c in cell_state[-1]:\n",
        "                  if c in char_lookup:\n",
        "                    encoded.append(char_lookup[c])   \n",
        "                  else:\n",
        "                    encoded.append(.5)    \n",
        "                lines.append(encoded)\n",
        "                cnt += 1\n",
        "\n",
        "train_data = torch.FloatTensor(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpNH9IKhsZqO",
        "colab_type": "code",
        "outputId": "b7b29865-cdec-4bed-91ef-c628099d7631",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = train_data\n",
        "# initialize network\n",
        "net = AutoEncoder()\n",
        "\n",
        "# learning rate (.001 - .0015 works well for Adam), number of epochs, optimizer\n",
        "lr = .0001\n",
        "epochs = 500\n",
        "batch_size = 20\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "'''(ii). Train auto-encoder'''\n",
        "for epoch in range(epochs):\n",
        "    for batch in range(9990 // batch_size):\n",
        "        # sample batch of size 20 from train data\n",
        "        indices = np.random.permutation(X.shape[0])[:batch_size]\n",
        "        train_X = X[indices]\n",
        "\n",
        "        # 2. zero gradient buffer\n",
        "        optimizer.zero_grad()\n",
        "        net.zero_grad()\n",
        "\n",
        "        # one pass to get expected output\n",
        "        latent, output = net(train_X)\n",
        "        loss = -net.loss(train_X, output)\n",
        "\n",
        "        #plotter.scalar_summary('Loss', loss, epoch * 10000 // batch_size + batch)\n",
        "\n",
        "        # update parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('epoch number : {}, loss : {}'.format(epoch, loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch number : 0, loss : -6787.7607421875\n",
            "epoch number : 1, loss : -12395.935546875\n",
            "epoch number : 2, loss : nan\n",
            "epoch number : 3, loss : nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-76a5d3966601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch number : {}, loss : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aK7J1guhWze",
        "colab_type": "code",
        "outputId": "fb11c8e6-40a5-4c99-b80c-21238eac83af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "def predict(x):\n",
        "  with torch.no_grad():\n",
        "    net_output = net(x)[1]\n",
        "    actual_output = (2 * net_output).round() / 2\n",
        "    return actual_output\n",
        "\n",
        "original = test_data\n",
        "output = predict(original)\n",
        "print('original ' + str(original))\n",
        "print('output ' + str(output))\n",
        "\n",
        "errors = torch.where(original - output > 0, torch.ones_like(original), torch.zeros_like(original))\n",
        "print(torch.sum(errors) / 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original tensor([[0.0000, 0.5000, 0.5000,  ..., 0.0000, 0.5000, 0.5000],\n",
            "        [0.0000, 0.5000, 0.5000,  ..., 0.0000, 0.5000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.5000,  ..., 0.5000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.5000, 0.0000,  ..., 0.5000, 0.0000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.5000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]])\n",
            "output tensor([[0.0000, 0.5000, 0.0000,  ..., 0.0000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.0000, 0.0000,  ..., 0.5000, 0.5000, 0.0000],\n",
            "        [0.5000, 0.0000, 0.5000,  ..., 0.0000, 0.0000, 0.5000],\n",
            "        ...,\n",
            "        [0.5000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5000],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.0000, 0.5000]])\n",
            "tensor(48.1180)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}